{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    NoopResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    "    ClipRewardEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "# env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "# env = gym.experimental.wrappers.RecordVideoV0(env, './video', episode_trigger=lambda t: t % 50 == 0, video_length=200)\n",
    "\n",
    "# observation = env.reset()\n",
    "# for _ in range(200):\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "#     if terminated or truncated:\n",
    "#         observation = env.reset()\n",
    "#         print(f\"Episodic return {info['episode']['r']}\")\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized environment\n",
    "# def make_env(gym_id, render_mode=False):\n",
    "#     def _thunk():\n",
    "#         env = gym.make(gym_id, render_mode=render_mode)\n",
    "#         env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "#         # env = gym.experimental.wrappers.RecordVideoV0(env, './video', episode_trigger=lambda t: t % 100 == 0, video_length=200)\n",
    "#         return env\n",
    "#     return _thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    nn.init.orthogonal_(layer.weight, std)\n",
    "    nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "def make_env(gym_id, render_mode=False, capture_video=False):\n",
    "    def thunk():\n",
    "        env = gym.make(gym_id, render_mode=render_mode)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            env = gym.experimental.wrappers.RecordVideoV0(env, './video', episode_trigger=lambda t: t % 50 == 0, video_length=2000)\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, shape=84)\n",
    "        env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStack(env, num_stack=4)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv([make_env('ALE/Breakout-v5', \"rgb_array\") for _ in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = envs.reset()\n",
    "for _ in range(200):\n",
    "    action = envs.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = envs.step(action)\n",
    "    if terminated.any() or truncated.any():\n",
    "        observation = envs.reset()\n",
    "        # print(f\"Episodic return {info}\")\n",
    "    envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (actor): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (critic): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(4, 32, kernel_size=8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64*7*7, 512)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = layer_init(nn.Linear(512, envs.single_action_space.n), std=0.01)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1.0)\n",
    "        # self.critic = nn.Sequential(\n",
    "        #     layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "        #     nn.Tanh(),\n",
    "        #     layer_init(nn.Linear(64, 64)),\n",
    "        #     nn.Tanh(),\n",
    "        #     layer_init(nn.Linear(64, 1), std=1.),\n",
    "        # )\n",
    "        \n",
    "        # self.actor = nn.Sequential(\n",
    "        #     layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "        #     nn.Tanh(),\n",
    "        #     layer_init(nn.Linear(64, 64)),\n",
    "        #     nn.Tanh(),\n",
    "        #     layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        # )\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(self.network(x / 255.0))\n",
    "        \n",
    "agent = Agent(envs).to(device)\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-05\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(agent.parameters(), lr=1e-3, eps=1e-5)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps = 128\n",
    "num_envs = 8\n",
    "total_timesteps = 1000000\n",
    "batch_size = num_envs * num_steps\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs = torch.Tensor(np.array(envs.reset()[0])).to(device)\n",
    "next_done = torch.zeros(num_envs).to(device)\n",
    "num_updates = total_timesteps // batch_size\n",
    "num_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_obs.shape: torch.Size([8, 4, 84, 84])\n",
      "agent.get_value(next_obs): tensor([[-0.0894],\n",
      "        [-0.1026],\n",
      "        [-0.0905],\n",
      "        [-0.0596],\n",
      "        [-0.0855],\n",
      "        [-0.0971],\n",
      "        [-0.0703],\n",
      "        [-0.0649]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "agent.get_value(next_obs).shape: torch.Size([8, 1])\n",
      "\n",
      "agent.get_action_and_value(next_obs):  (tensor([2, 1, 3, 2, 0, 1, 2, 3], device='cuda:0'), tensor([-1.3887, -1.3868, -1.3838, -1.3887, -1.3859, -1.3867, -1.3887, -1.3839],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>), tensor([1.3863, 1.3863, 1.3863, 1.3863, 1.3863, 1.3863, 1.3863, 1.3863],\n",
      "       device='cuda:0', grad_fn=<NegBackward0>), tensor([[-0.0894],\n",
      "        [-0.1026],\n",
      "        [-0.0905],\n",
      "        [-0.0596],\n",
      "        [-0.0855],\n",
      "        [-0.0971],\n",
      "        [-0.0703],\n",
      "        [-0.0649]], device='cuda:0', grad_fn=<AddmmBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(f'next_obs.shape: {next_obs.shape}')\n",
    "print(f'agent.get_value(next_obs): {agent.get_value(next_obs)}')\n",
    "print(f'agent.get_value(next_obs).shape: {agent.get_value(next_obs).shape}')\n",
    "print()\n",
    "print(\"agent.get_action_and_value(next_obs): \", agent.get_action_and_value(next_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m actions[step] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m     18\u001b[0m logprobs[step] \u001b[38;5;241m=\u001b[39m logprob\n\u001b[1;32m---> 20\u001b[0m next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m rewards[step] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m next_obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39marray(next_obs))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\gymnasium\\vector\\vector_env.py:204\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    {}\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:149\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m [], {}\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (env, action) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions)):\n\u001b[0;32m    143\u001b[0m     (\n\u001b[0;32m    144\u001b[0m         observation,\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminateds[i],\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncateds[i],\n\u001b[0;32m    148\u001b[0m         info,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminateds[i] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncateds[i]:\n\u001b[0;32m    152\u001b[0m         old_observation, old_info \u001b[38;5;241m=\u001b[39m observation, info\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\gymnasium\\wrappers\\frame_stack.py:179\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\gymnasium\\core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\gymnasium\\core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\gymnasium\\core.py:555\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\gymnasium\\core.py:461\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    459\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    460\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py:112\u001b[0m, in \u001b[0;36mEpisodicLifeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtariStepReturn:\n\u001b[1;32m--> 112\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_real_done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# check current lives, make loss of life terminal,\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# then update lives to handle bonus lives\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py:184\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_buffer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m--> 184\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(reward)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for update in range(1, num_updates+1):\n",
    "    \n",
    "    # Learning Rate Annealing Schedule\n",
    "    frac = 1.0 - (update - 1.0) / num_updates\n",
    "    lr_now = 1e-3 * frac\n",
    "    optimizer.param_groups[0]['lr'] = lr_now\n",
    "    \n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "        \n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs = torch.Tensor(np.array(next_obs)).to(device)\n",
    "        next_done = torch.Tensor(np.logical_or(terminated, truncated)).to(device)\n",
    "        \n",
    "        \n",
    "  \n",
    "        if next_done.any():\n",
    "            if info['final_info'][0] is not None:\n",
    "                # print(f'global_step: {global_step}')\n",
    "                # print(f\"Episodic return {info}\")\n",
    "                break\n",
    "            \n",
    "    # Generalized Advantage Estimation        \n",
    "    gamma = 0.99\n",
    "    gae_lambda = 0.95\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t+1]\n",
    "                nextvalues = values[t+1]\n",
    "            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values \n",
    "        \n",
    "    # Flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "    \n",
    "    # Minibatch Update\n",
    "    minibatch_size = batch_size // 4\n",
    "    update_epochs = 4\n",
    "    \n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            # print(f'start and end: {start} and {end}')\n",
    "        _, newlogproba, entropy, newvalues = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "        logratio = newlogproba - b_logprobs[mb_inds]\n",
    "        ratio = logratio.exp()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            clip_coef = 0.1\n",
    "            old_approx_kl = (-logratio).mean()\n",
    "            approx_kl = ((ratio - 1.0) * logratio).mean()\n",
    "            clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean()]\n",
    "        \n",
    "        # Advantages Normalization\n",
    "        mb_advantages = b_advantages[mb_inds]\n",
    "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "        \n",
    "        # Clipped Surrogate Objective\n",
    "        clip_coef = 0.1\n",
    "        pg_loss1 = -mb_advantages * ratio\n",
    "        pg_loss2 = -mb_advantages * torch.clamp(ratio, 1.0 - clip_coef, 1.0 + clip_coef)\n",
    "        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "        \n",
    "        # Value Loss CLipping \n",
    "        v_loss_unclipped = (newvalues - b_returns[mb_inds]) ** 2\n",
    "        v_clipped = b_values[mb_inds] + torch.clamp(newvalues - b_values[mb_inds], -clip_coef, clip_coef)\n",
    "        v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "        v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "        v_loss = 0.5 * v_loss_max.mean()\n",
    "        \n",
    "        # Entropy Loss\n",
    "        ent_coef = 0.01\n",
    "        vf_coef = 0.5\n",
    "        entropy_loss = entropy.mean()\n",
    "        loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Global Gradient CLipping\n",
    "        max_grad_norm = 0.5\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        optimizer.step()   \n",
    "    \n",
    "    # Target KL Diverngence\n",
    "    # target_kl = 0.015\n",
    "    # if approx_kl > target_kl:\n",
    "    #     print(f'Early stopping at step {step} due to reaching max kl: {approx_kl}')\n",
    "    #     break\n",
    "\n",
    "y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "var_y = np.var(y_true)\n",
    "explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "print(f'Learning Rate {optimizer.param_groups[0][\"lr\"]}')\n",
    "print(f'Value Loss: {v_loss.item()}')\n",
    "print(f'Policy Loss: {pg_loss.item()}')\n",
    "print(f'Entropy Loss: {entropy_loss.item()}')\n",
    "print(f'Approx KL: {approx_kl.item()}')\n",
    "print(f'Clipfrac {torch.stack(clipfracs).mean()}')\n",
    "print(f'Explained Var: {explained_var}')\n",
    "print(f'Elapsed time (s): {time.time() - start_time}')\n",
    "\n",
    "torch.save(agent.state_dict(), 'ppo_breakout.pth')\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\gym_env\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-50.mp4.\n",
      "Moviepy - Writing video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-50.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-50.mp4\n",
      "Moviepy - Building video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-100.mp4.\n",
      "Moviepy - Writing video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-100.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-100.mp4\n",
      "Moviepy - Building video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-150.mp4.\n",
      "Moviepy - Writing video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-150.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-150.mp4\n",
      "Moviepy - Building video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-200.mp4.\n",
      "Moviepy - Writing video c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-200.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\andre\\dev\\ucdavis\\spring24\\ecs170\\project\\ProximalPolicyOptimization\\video\\rl-video-episode-200.mp4\n"
     ]
    }
   ],
   "source": [
    "num_envs = 1\n",
    "# Vectorized environment\n",
    "envs = gym.vector.SyncVectorEnv([make_env('ALE/Breakout-v5', \"rgb_array\", True)])\n",
    "\n",
    "# Load the model\n",
    "agent = Agent(envs).to(device)\n",
    "agent.load_state_dict(torch.load('ppo_breakout.pth'))\n",
    "agent.eval()\n",
    "\n",
    "obs = torch.Tensor(np.array(envs.reset()[0])).to(device)\n",
    "next_obs = obs\n",
    "next_done = torch.zeros(num_envs).to(device)\n",
    "\n",
    "for _ in range(10000):\n",
    "    with torch.no_grad():\n",
    "        action, _, _, _ = agent.get_action_and_value(next_obs)\n",
    "    next_obs, _, next_term, next_trun, info = envs.step(action.cpu().numpy())\n",
    "    next_obs = torch.Tensor(np.array(next_obs)).to(device)\n",
    "    if next_term.any() or next_trun.any():\n",
    "        next_obs = torch.Tensor(np.array(envs.reset()[0])).to(device)\n",
    "        # print(f\"Episodic return {info}\")\n",
    "\n",
    "    envs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
