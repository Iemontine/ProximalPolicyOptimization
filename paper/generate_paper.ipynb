{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "from ast import Sub\n",
    "from math import e\n",
    "from pylatex.utils import italic, bold, NoEscape\n",
    "from pylatex import Document, Section, Subsection, Subsubsection, Description, Itemize, Command, Tabular, Math, TikZ, Axis, Plot, Figure, Matrix, Alignat, NewPage, NewLine, Enumerate, Center, Package\n",
    "from pylatex.section import Paragraph, Subparagraph, Chapter\n",
    "from pylatex.utils import italic\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "# =================================[PAGE SETUP]=================================\n",
    "geometry_options = { \"margin\": \"0.8in\", \"includeheadfoot\": False }\n",
    "doc = Document(geometry_options=geometry_options, lmodern = True)\n",
    "doc.packages.append(Package('amssymb'))\n",
    "# ====================================[TITLE]===================================\n",
    "doc.preamble.append(Command('title', NoEscape(r'Gameplaying AI: Implementing Proximal Policy Optimization\\\\ \\large Introduction to Artificial Intelligence Project\\\\ \\large ECS 170 Spring 2024')))\n",
    "doc.preamble.append(Command('author', NoEscape(r'Darroll Saddi\\textsuperscript{1}, Andrew Yeow\\textsuperscript{1}, Christine Morataya\\textsuperscript{2}, Julia Heiler\\textsuperscript{1}, Ryan Li\\textsuperscript{1}, Steven Yi\\textsuperscript{1}\\\\ \\small\\textsuperscript{1}University of California, Davis - Computer Science\\\\\\small\\textsuperscript{2}University of California, Davis - Cognitive Science')))\n",
    "doc.preamble.append(Command('date', NoEscape(r'\\today')))\n",
    "doc.append(NoEscape(r'\\maketitle'))\n",
    "# ==================================[ABSTRACT]==================================\n",
    "doc.append(NoEscape(r'\\begin{abstract}This report documents a retrospective reimplementation of proximal policy optimization (PPO), for study purposes. Our goal was to not only use the algorithm to successfully train an artificial intelligence to play a semi-complex 2D video game, but through this practical experience, discover how to use, articulate, and implement reinforcement learning (RL) algorithms like PPO. We also present this report as an insightful and educational resource for those interested in reinforcement learning, studying our design process, and/or recreating our training environment.\\end{abstract}'))\n",
    "# ================================[INTRODUCTION]================================\n",
    "with doc.create(Section('Introduction')):\n",
    "    doc.append(NoEscape(r'Reinforcement learning (RL) has emerged as a powerful paradigm for artificial intelligence and artificial learning. Proximal Policy Optimization (PPO) is a notable advancement in this field, being a versatile algorithm that has shown significant promise across various domains, especially continuous state spaces. This report documents the extensive learning and utilization process of OpenAI’s PPO implementation to train an agent to play the classic video game, Sonic the Hedgehog\\textsuperscript{TM}, with a particular focus on optimizing reward functions and hyperparameters by analyzing trained agent behaviors. In other words, our objectives were to increase the broader community’s understanding of the RL training process/environment, and how to use an algorithm to solve a given problem. Video games and other artificial environments are an ideal environment to practice or test RL algorithms as they are easy to understand and analyze (e.g. agent is learning to go to the left when the goal is on the right → something is wrong with the reward function!). Through this project, we aim to provide an insightful and educational resource for the practical challenges associated with utilizing and implementing PPO. By documenting our development process, challenges, and solutions, we hope to contribute to ease of access and understanding of introductory resources for reinforcement learning, which is an area we believe requires further development.'))\n",
    "    # doc.append(NoEscape(r'Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that has been used to train agents in continuous action space environments, including video games. It is a policy gradient method that is designed to be simple to implement and computationally efficient. Being a on-policy method, the algorithm learns a policy to make decisions in the environment. This is different from an off-policy method, which learns the value of the optimal policy independently of the agent\\'s action. In this project, we reimplemented PPO and used it to train an agent to play and complete levels in Sonic the Hedgehog^\\(TM\\). On a high level, PPO works by choosing an action for the agent to take, then observing the resultant state and reward. Then, an estimate for the advantage gain is computed (see GAE), which measures how much better the action was compared to the average action at that state. The policy is then updated using a special objective function to prevent the policy from updating too mcuh on a single episode by penalizing if the new policy deviates too much from the original policy, ensuring stability. These steps repeat until training is complete. Here is an example of a citation\\footnote{This is a reference}. Here is another reference\\footnote{This is another reference}. This is how I would cite the first reference again^\\(1\\).'))\n",
    "    # More context on PPO needed above\n",
    "    # TODO: Be sure to draw on multiple sources for this introduction. Explain motivation for project.\n",
    "# ================================[BACKGROUND]==================================\n",
    "with doc.create(Section('Background')):\n",
    "    with doc.create(Subsection('Use-Cases')):\n",
    "        doc.append(NoEscape(r'Proximal policy optimization (PPO) is a versatile reinforcement learning algorithm that can be used in a variety of environments. It is particularly well-suited for continuous state spaces, which makes it a good candidate for training agents in games like Sonic the Hedgehog\\textsuperscript{TM}.'))\n",
    "        # TODO: Talk about the build-up to PPO, and how it excels over its predecessors.\n",
    "    with doc.create(Subsection('PPO Algorithm')):\n",
    "        # TODO: PLS REWRITE + ADD REFERENCES\n",
    "        with doc.create(Paragraph('Actor-Critic')):\n",
    "            doc.append(NoEscape(r'PPO is an actor-critic algorithm, which means that it uses two neural networks to approximate the policy and value functions. In actor-critic models, the actor controls the action the agent takes (by executing the policy) and the critic returns a value that represents how good or bad that action is in the current state. The actor network takes the current state as input and outputs a probability distribution over possible actions. The critic network takes the current state as input and outputs an estimate of the value of the state. Both use the metric of \\(advantage\\) to update weights, which measures how much better taking a particular action is compared to the average action.'))\n",
    "        with doc.create(Paragraph('Ratio')):\n",
    "            doc.append(r'The ratio is the new policy divided by the old policy. Values < 1 mean the action, a at state, s is more likely (or better) in the new policy than the old one. When the ratio is between 0 and 1 the action is less likely (or worse) in the new policy than the old one.')\n",
    "        with doc.create(Paragraph('Generalized Advantage Estimation (GAE)')):\n",
    "            doc.append(NoEscape(r\"This function measures how good the current state is using advantage, a measure of how much better an action is than the average action. Advantage it is used to determine how to update the policy to improve performance. At a high level, GAE is a variation of advantage estimation whose goal is to significantly reduce variance while maintaining a tolerable level of bias, which in turn will stabilize the training of the agent.\"))\n",
    "        with doc.create(Paragraph('Surrogate Objective Function')):\n",
    "            doc.append(NoEscape(r'The key to PPO is the surrogate objective function, which helps maximize the probability of taking an action that may eventually lead to high reward. Its main purpose is to keep updates within a trust region, using clipping.'))\n",
    "        with doc.create(Paragraph('Clipping')):\n",
    "            doc.append(NoEscape(r\"The key idea of PPO, clipping, prevents the policyh from changing too much between updates, thereby improving training stability. This is accomplished by clipping the ratio of the new policy to the old plicy within a specific range, [1 - $\\epsilon$, 1 + $\\epsilon$]. This range acts like a safety zone, ensuring that the updates are small and controlled. If the ratio goes outside this range, the update is clipped, or limited, so the change isn't too big. In short, clipping is to make conservative updates to the model.\"))\n",
    "        with doc.create(Paragraph('Overall')):\n",
    "            doc.append(r\"Altogether, we take the minimum of the ratio multiplied by the advantage and the clipped ratio multiplied by the advantage. To reiterate, the main purpose of PPO is to limit policy changes. This is implemented through the use of the clipped objective function and generalized advantage estimation, allowing PPO to have more stable training than vanilla policy gradient methods.\")\n",
    "            with doc.create(Alignat(numbering=False, escape=False)) as agn:\n",
    "                agn.append(r\"\"\"L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\"\"\")\n",
    "            with doc.create(Alignat(numbering=False, escape=False)) as agn:\n",
    "                agn.append(NoEscape(r\"\"\"Â_t = \\sum_{\\ell=0}^{\\infty} (\\gamma \\lambda)^{\\ell} \\delta_{t+\\ell}\"\"\"))\n",
    "            # TODO: invest in some visualization\n",
    "            # TODO: include an image that connects every part of the algorithm\n",
    "            \"\"\"here is just an example of what math things can be displayed using pylatex\"\"\"\n",
    "            # a = np.array([[100, 10, 20]]).T\n",
    "            # M = np.matrix([[2, 3, 4],\n",
    "            #                 [0, 0, 1],\n",
    "            #                 [0, 0, 2]])\n",
    "            # doc.append(Math(data=['2*3', '=', 9]))\n",
    "            # with doc.create(Tabular('rc|cl')) as table:\n",
    "            #     table.add_hline()\n",
    "            #     table.add_row((1, 2, 3, 4))\n",
    "            #     table.add_hline(1, 2)\n",
    "            #     table.add_empty_row()\n",
    "            #     table.add_row((4, 5, 6, 7))\n",
    "            # doc.append(Math(data=[Matrix(M), Matrix(a), '=', Matrix(M * a)]))\n",
    "with doc.create(Section('Development Process')):\n",
    "    with doc.create(Subsection('Environment')):\n",
    "        with doc.create(Subsubsection('Framework')):\n",
    "            doc.append(NoEscape(r\"After discovering Gym-Retro, which offers a variety of learning scenarios to choose from, we developed a proof-of-concept of training with a different algorithm (Deep Q-Learning). However, due to the outdatedness of some of the libraries and all its dependencies, we opted to search for a different approach that supported our desired versions of Python, Pytorch, and Gymnasium (RL environment loader). After deliberation on the frameworks and games to work with, we decided upon Sonic the Hedgehog\\textsuperscript{TM} as the game to train an agent on, and Gymnasium x Stable-Retro, a ROM-loading and RL training framework. This game was chosen for its relative simplicity (gotta go fast!..to win), for its semi-complex continuous state space, and the fact that there are pre-existing benchmarks for games of its kind. Stable-Retro allows us to load and interact with video game ROMs as the training environment, and provides a variety of pre-existing tools for reading information from retro video games. These are necessary for developing an RL training environment as well as providing reward to the agent. Not only do these libraries have better dependency support, this approach also allowed us to focus on the setup for training and the training itself, removing the need for us to painstakingly target specific RAM values from the game process. Such vital information used to design the reward function and training restrictions include the current level, Sonic’s position, and the number of lives.\"))\n",
    "        with doc.create(Subsubsection('Wrappers')):\n",
    "            doc.append(NoEscape(r'To simplify the training environment, I (Darroll) used wrappers to preprocess a variety of features about the game state before it is fed into the algorithm. These wrappers are used to reduce the computational cost of training and to provide the agent with a more informative view of the environment. The following wrappers were implemented:'))\n",
    "            with doc.create(Description()) as desc:\n",
    "                desc.add_item(\"Rewarder\", r\"Allows the specification of a custom reward function, in addition to timing resets (e.g. go back to start of level, reset all tracked variables, and administer negative reward when Sonic dies).\")\n",
    "                desc.add_item(\"Observer\", r\"Feeds what is on the screen into a convolutional neural network to easily map the game state to agent, as opposed to manually reading the game state from RAM values. This proved to be crucial for training, as it allowed for agents to learn an association between map features and an action to respond to them with.\")\n",
    "                desc.add_item(\"Frame Stacker\", r\"Stacks the last 8 frames together to provide vectors between current and previous states to the CNN (crucial, allows for improved learning of jumping timing such as early jumping over obstacles).\")\n",
    "                desc.add_item(\"Action Mapper\", r\"Discretizes the action space to a limited number of one-hot-encoded actions (e.g. move right, jump, move left), as opposed to completely random controller input combinations, thereby decreasing input complexity and allowing for faster learning.\")\n",
    "                desc.add_item(\"Frame Skipper\", r\"Periodically skips game frames to allow for faster training, further reducing complexity as less frames are processed overall.\")\n",
    "                desc.add_item(\"Multiprocess Vectorizer\", r\"Allows for training multiprocessing, which speeds up training by allowing for multiple agent instances to investigate solutions to obstacles concurrently.\")\n",
    "    with doc.create(Subsection(('Reward Function'))):\n",
    "        doc.append(NoEscape(r\"For context, Stable-Baselines3 is a library that provides a variety of implementations of reinforcement learning algorithms, including PPO. Using OpenAI’s Stable-Baselines3 version of PPO, I abstracted the algorithm to focus on the training environment and the reward function.\"))\n",
    "        with doc.create(Subsubsection('Note about Evaluation Metrics')):\n",
    "            doc.append(NoEscape(r\"I should also mention that the metric I used to estimate the success of the agent are the win/loss ratio and the average level completion time. Furthermore, any mention of the quality of the agent’s problem-solving \\& exploration abilities were essentially measured by observing two things from trained models:\"))\n",
    "            with doc.create(Enumerate()) as enum:\n",
    "                enum.add_item(r\"Whether the agent learned to jump over obstacles, which I used to approximate the divide between exploration and exploitation phases of training, and whether the transition was too late or early.\")\n",
    "                enum.add_item(r\"Whether the agent learned to complete a circular loop in the middle of the level, which I used to measure exploration ability as it required the agent to maintain rightward momentum to solve.\")\n",
    "            doc.append(r\"The latter was an interesting problem to solve, which will be discussed later but varied drastically from model to model depending on hyperparameter values and the reward function.\")\n",
    "        with doc.create(Paragraph('Velocity')):\n",
    "            doc.append(NoEscape(r\"Initial tests with velocity-based rewards were unsuccessful. Calculated as $\\Delta x$, velocity-based rewards led to reward farming as well as policies that preferred to move around in ways that acquired bursts of speed. Even after tuning the reward to account only for rightward velocity, the agent still failed to consistently beat the level. This may have been due to the fact that this reward had a small relation to completing the level, which would would explain the agent's inability to learn how to do so. In other words, failing to propose a proper problem statement, can cause the problem to be solved in unexpected ways.\"))\n",
    "        with doc.create(Paragraph('Progress')):\n",
    "            doc.append(NoEscape(r\"Based on the failures of the previous reward function as well as on pre-existing research, I moved on to a progress-based reward function. This function rewards the agent for making rightward progress through the level, and is a better heuristic as it actually relates to the task of completing the level. This reward function was successful in training the agent to complete the level, but it was not able to train the agent to complete the loop. It was at this point where I additionally implemented punishments (negative rewards) for the agent losing a life, as well as massive static rewards for completing the level. This reward function yielded good results, and the agent was able to at least make progress up to the loop. However, I observed the agents getting stuck in a local minimum by repeatedly jumping rightwards into the loop, and would never learn how to complete it. I believe this was due to agents entering an ’exploitation’ phase of training too early, and would continue to repeat the behavior of ’jumping to the right’ that was successfully solving most obstacles before the loop. Furthermore, completing the loop with progress as the reward is problematic as completing it incurs leftward movement, where no reward would be given. It was clear I needed to tune the hyperparameters to increase the agent’s exploration ability, to not only have it learn to maintain its momentum but to additionally explore this segment of no-reward within the loop.\"))\n",
    "    with doc.create(Subsection('Hyperparameter Tuning')):\n",
    "        doc.append(NoEscape(r\"A roadblock in development was figuring out good hyperparameters and reward magnitudes to train the model with. Depending on the values chosen, the outcomes varied greatly. At some point, I also spent time investigating using the Optuna library to perform automated hyperparameter tuning, but due to the sheer complexity and required length of training (several hours), this approach was not feasible under time constraints as it would require multiple full training cycles to determine the optimized values. What follows is a brief description of what values I converged upon, and my intuition for why they worked.\"))\n",
    "        with doc.create(Description()) as desc:\n",
    "            desc.add_item(\"Discount factor\", r\"0.95, arbitrarily chosen from training standards\")\n",
    "            desc.add_item(\"Bias trade-off vs variance factor\", r\"0.985, arbitrarily chosen from training standards\")\n",
    "            desc.add_item(\"Clipped surrogate objective\", r\"0.75, arbitrarily chosen from training standards\")\n",
    "            desc.add_item(\"Entropy\", r\"0.30, mild magnitude to encourage random actions to explore solutions to completing the loop, without being excessively high which prevented the agent from learning from successes (observed when the agent jumps around randomly even after several hundred thousand timesteps).\")\n",
    "            desc.add_item(\"Learning Rate\", r\"0.00075, values that were too high (0.1, 0.001, 0.0001) caused the agent to enter the exploitation phase of training too early, evident by the agent performing at a local maximum (preferring to get stuck at the loop to farm progress rewards during the reset from taking too long). Values that were too low (0.00001) or decreased learning speed without significant benefit, so a slightly higher value was chosen to increase speed while maintaining exploration.\")\n",
    "            desc.add_item(\"Timesteps before updating policy\", r\"512, this value measures the number of timesteps per agent before the policy is updated. 4096 was originally used, as I believed this would allow the agent longer stretches of forward progress with large amounts of positive reward. Although agents learned to beat early areas of the map consistently after around 500,000 timesteps, I discovered that lowering the value to 512 decreased this to a mere 50,000-100,000 timesteps. I interpreted this as being due to each section of the map essentially now being solved in smaller pieces with 512, whereas with 4096, much larger portions of the level are presented to the neural network as larger problems.\")\n",
    "    with doc.create(Subsection('Multi-Pass Training')):\n",
    "        with doc.create(Subsubsection('Adjusting Reward/Punishment Weights')):\n",
    "            doc.append(NoEscape(r\"At this point in development, I arbitrarily chose weights for agent progress rewards, death punishments, and level complete rewards. As it turns out however, these each had massive impacts on the quality of training and produced widely varying models. Depending on these magnitudes, policies could develop that never learned to complete the loop, opting again to farm the initial progress rewards from starting the level, or even in the worst-case of my observation, be completely unable to learn to jump over obstacles. Based on this, I lowered the progress rewards to be calculated as $\\Delta x / 100$. This produces rewards as decimals in the range of 0.0 and 0.5. The resultant training was faring far better, and it seemed that keeping rewards low, but not insignificant, was important for agents to develop a mapping between making forward progress and getting closer to the true goal which is the end of the level. It follows that level completion reward and death punishment magnitudes converged on being set to 50 and -1 respectively, both being subject to change as I continued my optimization efforts.\"))\n",
    "        with doc.create(Subsubsection('Multi-Pass')):\n",
    "            doc.append(r\"At this point, I was getting results, but they weren't satisfactory.\")\n",
    "            with doc.create(Figure(position='th!')) as fig:\n",
    "                image_filename = \"Pass1_15M_1.png\"\n",
    "                fig.add_image(image_filename, width='497px')\n",
    "                fig.add_caption('Winrate, completion time progression after 1.5 million timesteps, one-shot training')\n",
    "            doc.append(r\"Figure 1 shows that as training progressed, the winrate, even after a period of volatile failure that occurs from early training in which the likelihood of losing a life is very high, ranges between 43.94% and 49.33%. In other words, by the end of training the model with the above reward/punishment magnitudes, the resultant policy is successful less than roughly 50% of the time. Furthermore when it is able to complete the level, the 5 lowest completion times are relatively low. However, the average time is very poor, at around 1 minute and 50 seconds, which means completion times are inconsistent given the lower and middle bounds. Keep in mind the world record completion of this level by a human is around 25 seconds. This led me to develop a new multipass method, in which each pass of three passes utilizes a different reward functions, working as follows:\")\n",
    "            with doc.create(Center()) as centered_table:\n",
    "                with doc.create(Tabular('|c|c|c|c|c|')) as table:\n",
    "                    table.add_hline()\n",
    "                    table.add_row(NoEscape(r'\\textbf{Pass \\# (500k timesteps)}'), NoEscape(r'\\textbf{Progress}'),  NoEscape(r'\\textbf{Winning}'), NoEscape(r'\\textbf{Death}'), NoEscape(r'\\textbf{Lack of progress}'))\n",
    "                    table.add_hline()\n",
    "                    table.add_row('Pass 1 (original setup)', NoEscape(r'$\\Delta x / 100$'), 50, '0', '0.00')\n",
    "                    table.add_hline()\n",
    "                    table.add_row('Pass 2', NoEscape(r'$\\Delta x / 100$'), 50, '-1', '-0.01')\n",
    "                    table.add_hline()\n",
    "                    table.add_row('Pass 3', NoEscape(r'$\\Delta x / 100$'), 'max(0, current timesteps/1000)', '-2', '-0.02')\n",
    "                    table.add_hline()\n",
    "            doc.append(NoEscape(r\"\"\"At this point, I was skeptical of an implementation that 'hard-coded' a learning progression, as opposed training with an ideal reward function and ideal hyperparameters. I only took this approach because I thought at the time that punishing on no-progress was not possible due to initial failures with this method that turned out to be due to poor reward magnitudes and hyperparemter choices. My intuition for what was wrong, was that the setup lacked a heuristic emulating the learning curve a human might traverse when learning to beat a video game level quickly; the agents were able to learn to beat the level, but would rarely optimize for time, which I believed could be achieved by punishing on a lack of progress once the agent knew how to beat the level. While this proved to be wrong, here were the results:\"\"\"))\n",
    "            with doc.create(Figure(position='th!')) as fig:\n",
    "                image_filename = \"multipass.png\"\n",
    "                fig.add_image(image_filename, width='497px')\n",
    "                fig.add_caption('multipass implementation; winrate progression, winrate frequency, overall winrate')\n",
    "            doc.append(r\"The first two charts in Figure 2 provide context on how each of the three pass's win percentages and frequencies compare, as well as how many wins/losses each pass was able to obtain. Additionally, by comparing Winrate Progression from Figure 1 and Overall Winrate in Figure 2, it is clear that the performance of this 3-Pass implementation is a significant improvement. The winrate, stabilized after ignoring the volatile exploration phase (estimated to be within the first 600 wins/losses) is at least 82.94%. Compared to the 49.33% from the preceding setup, this is a 68.13% improvement in winrate, which is quite significant. To prove this methodology is in any way ideal, I decided to attempt training 1.5M timesteps on each component Pass separately, in order to examine their performance and draw comparisons, here are the results:\")\n",
    "            #  This is where I realized that the underlying problem was that reward function used in Pass 1 was simply poorly designed, all on the basis of the whether a lack of progress was punished.\n",
    "            with doc.create(Figure(position='th!')) as fig:\n",
    "                image_filename = \"punish_v_nopunish_winrates_levelcompletion.png\"\n",
    "                fig.add_image(image_filename, width='490px')\n",
    "            with doc.create(Figure(position='th!')) as fig:\n",
    "                image_filename = \"punish_v_nopunish_wintimes.png\"\n",
    "                fig.add_image(image_filename, width='300px')\n",
    "                fig.add_caption('Comparing results from training with and without punishing on lack of progress')\n",
    "            doc.append(NewPage())\n",
    "            doc.append(r\"Resultant winrate from training while punishing lack of progress, compared to training in multiple passes with varying reward functions, proved to be slightly better for 1.5 million timesteps. Where multi-pass had winrate in the range of 65.86%-82.34%, a single pass with Pass 3's reward function, resulted in a winrate in the range of 77.20%-83.05%. Similarly, the wincount also went up, from 267+892+1072=2231 for multipass to a flat 3037 wins from the extended Pass 3. The fastest completion times seemed to converge to roughly the same efficiency between both methodologies. Finally, although resultant wintimes for both methods are roughly the same (31.5-31.8 seconds), the average was driven down nearly 5 seconds. For further comparison, the charts also include the results from Figure 1. Clearly, Pass 3's reward function is a better fit for the problem, and Pass 1's reward function was detrimental to training, driving all the metrics down due to its weakness. This is significant, as it means that me hard-coding a learning progression may have been unnecessary.\")\n",
    "with doc.create(Section('Results/Reflections')):\n",
    "    with doc.create(Subsection('Reinforcement Learning as True Learning')):\n",
    "        doc.append(NoEscape(r\"Throughout the development of this project, I often intuited how to produce more efficient training by referencing how the agent was performing in the environment, in other words observing the agent’s behavior and adjusting hyperparameters accordingly. For example, I noticed that the agent was getting stuck at the loop, so I increased the entropy to encourage more exploration of the problem and decreased the number of timesteps before updating the policy to allow the agent to learn to beat the level in smaller pieces. Another example was how I designed the reward function. I often thought of how a human might learn to beat a level of a video game quickly. I imagined they would traverse a learning curve, first needing to learn to beat the level, then optimizing after that and self-punishing when dying or not moving faster. This led me to implement the multi-pass implementation that produced positive results. Although the multi-pass method proved to be worse than one-shot generally, it was what led me back to one-shot training in the first place. This goes to show the power of reinforcement learning algorithms like PPO to mimic true learning, likely going through the same learning curve that I attempted to hard code, in its own way. These high-level, ’black-box’ observations were crucial to the success of the project, as they allowed me to find intuitive learning heuristics, leading to the 90\\% accuracy of the model. It is also clear that the focus should undoubtedly be on training and learning, rather than attempting to hard-code human-centric implementation. I believe that similar simplifications and abstractions of the inner workings of PPO and similar algorithms must be taken in other reinforcement learning projects. I further believe that this is a testament to the power of reinforcement learning as an algorithm reflecting true learning.\"))\n",
    "        # TODO: test training with pass 1 only for 1,500,000 timesteps and pass 3 for 1,500,000 timesteps\n",
    "with doc.create(Section('GitHub')):\n",
    "    doc.append(r\"A repository containing the code used in this project can be found at:\")\n",
    "    doc.append(NewLine())\n",
    "    doc.append(NewLine())\n",
    "    doc.append(r\"https://github.com/Iemontine/ProximalPolicyOptimization.\")\n",
    "doc.append(NewPage())  # Insert a page break\n",
    "with doc.create(Section('Contributions')):\n",
    "    with doc.create(Subsection('Darroll Saddi')):\n",
    "        doc.append(r'Set up training environment, including integrating libraries & frameworks. Performed inference- and intuition-based hyperparameter tuning to produce training results. Designed reward function, investigating and testing multiple implementation styles. Implemented metric-tracking during training and wrote plotting code. Wrote Development Process and Results/Reflections for write-up & GitHub documentation. Contributed training/RL research and project direction.')\n",
    "    with doc.create(Subsection('Julia Heiler')):\n",
    "        doc.append(r'Contributed to documentation; presentation, write-ups, slide deck. Translated all documentation into a speech script, ensuring coherence and cohesion throughout. Contributed PPO research and project direction.')\n",
    "    with doc.create(Subsection('Andrew Yeow')):\n",
    "        doc.append(r'Implemented proof of concept with DQL. Articulated PPO for communication and explanation purposes.')\n",
    "    with doc.create(Subsection('Steven Yi')):\n",
    "        doc.append(r\"Refactored OpenAI's proximal policy optimization implementation to aid with understanding of the algorithm. Contributed to documentation; write-ups.\")\n",
    "    with doc.create(Subsection('Christine Morataya')):\n",
    "        doc.append(r'Contributed to documentation; presentation, write-ups, slide deck.')\n",
    "    with doc.create(Subsection('Ryan Li')):\n",
    "        doc.append(r'Contributed to documentation; write-ups.')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        doc.generate_pdf('output', clean_tex=False)\n",
    "        print(\"Completed!\")\n",
    "        break\n",
    "    except:\n",
    "        clear_output()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
