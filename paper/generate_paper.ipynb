{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "from ast import Sub\n",
    "from pylatex.utils import italic, bold, NoEscape\n",
    "from pylatex import Document, Section, Subsection, Subsubsection, Command, Tabular, Math, TikZ, Axis, Plot, Figure, Matrix, Alignat, NewPage, NewLine\n",
    "from pylatex.section import Paragraph, Subparagraph, Chapter\n",
    "from pylatex.utils import italic\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "# =================================[PAGE SETUP]=================================\n",
    "geometry_options = { \"margin\": \"1in\", \"includeheadfoot\": False }\n",
    "doc = Document(geometry_options=geometry_options, lmodern = True)\n",
    "# ====================================[TITLE]===================================\n",
    "doc.preamble.append(Command('title', NoEscape(r'Gameplaying AI: Reimplementing Proximal Policy Optimization\\\\ \\large Introduction to Artificial Intelligence Project\\\\ \\large ECS 170 Fall 2024')))\n",
    "doc.preamble.append(Command('author', NoEscape(r'Darroll Saddi^\\(1\\), Andrew Yeow^\\(1\\), Christine Morayata^\\(?\\), Julia Heiler^\\(?\\), Ryan Li^\\(1\\), Steven Yi^\\(?\\)\\\\ \\small^\\(1\\)University of California, Davis - Computer Science\\\\\\small^\\(2\\)University of California, Davis - Cognitive Science')))\n",
    "doc.preamble.append(Command('date', NoEscape(r'\\today')))\n",
    "doc.append(NoEscape(r'\\maketitle'))\n",
    "# ==================================[ABSTRACT]==================================\n",
    "doc.append(NoEscape(r'\\begin{abstract}This report documents a retrospective reimplementation of proximal policy optimization (PPO), performed at UC Davis for study purposes. Our goal was to not only use the algorithm to succesfully train a model to play Sonic the Hedgehog^\\(TM\\), but to additionally discover how to use, articulate, and implement reinforcement learning algorithms while being able to communicate PPO and reinforcement learning through practical experience. We also present this report as an insightful and educational resource for those interested in reinforcement learning and/or recreating our training environment.\\end{abstract}'))\n",
    "# ================================[INTRODUCTION]================================\n",
    "with doc.create(Section('Introduction')):\n",
    "    doc.append(NoEscape(r'Insert problem statement, motivation, and general introduction to the project.'))\n",
    "    # doc.append(NoEscape(r'Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that has been used to train agents in continuous action space environments, including video games. It is a policy gradient method that is designed to be simple to implement and computationally efficient. Being a on-policy method, the algorithm learns a policy to make decisions in the environment. This is different from an off-policy method, which learns the value of the optimal policy independently of the agent\\'s action. In this project, we reimplemented PPO and used it to train an agent to play and complete levels in Sonic the Hedgehog^\\(TM\\). On a high level, PPO works by choosing an action for the agent to take, then observing the resultant state and reward. Then, an estimate for the advantage gain is computed (see GAE), which measures how much better the action was compared to the average action at that state. The policy is then updated using a special objective function to prevent the policy from updating too mcuh on a single episode by penalizing if the new policy deviates too much from the original policy, ensuring stability. These steps repeat until training is complete. Here is an example of a citation\\footnote{This is a reference}. Here is another reference\\footnote{This is another reference}. This is how I would cite the first reference again^\\(1\\).'))\n",
    "    # More context on PPO needed above\n",
    "    # TODO: Be sure to draw on multiple sources for this introduction. Explain motivation for project.\n",
    "# ================================[BACKGROUND]==================================\n",
    "with doc.create(Section('Background')):\n",
    "    with doc.create(Subsection('Use-Cases')):\n",
    "        doc.append(NoEscape(r'PPO is a versatile algorithm that can be used in a variety of environments. It is particularly well-suited for continuous action spaces, which makes it a good candidate for training agents in games like Sonic the Hedgehog^\\(TM\\).'))\n",
    "        # TODO: Talk about the build-up to PPO, and how it excels over its predecessors.\n",
    "    with doc.create(Subsection('PPO Implementation')):\n",
    "        # TODO: PLS REWRITE + ADD REFERENCES\n",
    "        with doc.create(Paragraph('Actor-Critic')):\n",
    "            doc.append(NoEscape(r'PPO is an actor-critic algorithm, which means that it uses two neural networks to approximate the policy and value functions. The actor network takes the current state as input and outputs a probability distribution over possible actions. The critic network takes the current state as input and outputs an estimate of the value of the state. Both use the metric of \"advantage\" to update weights, which measures how much better taking a particular action is compared to the average action.'))\n",
    "        with doc.create(Paragraph('Generalized Advantage Estimation (GAE)')):\n",
    "            doc.append(NoEscape(r'This function \\'how good is the current state\\'. It is used to calculate the advantage of taking an action in a given state, which is then used to update the policy. The advantage is a measure of how much better an action is than the average action, and it is used to determine how to update the policy to improve performance.'))\n",
    "        with doc.create(Paragraph('Surrogate Objective Function')):\n",
    "            doc.append(NoEscape(r'The key to PPO is the surrogate objective function, which helps maximize the probability of taking an action that may eventually lead to high reward. Its main purpose is to keep updates within a trust region, using clipping (see below).'))\n",
    "        with doc.create(Paragraph('Clipping')):\n",
    "            doc.append(NoEscape(r'This is a technique used to prevent the policy from changing too much between updates. It is used to ensure that the policy does not change too much between updates, which would normally lead to training instability. It does this clipping the probability ratio between the new and old policies.'))\n",
    "            # TODO: invest in some visualization\n",
    "            # TODO: include an image that connects every part of the algorithm\n",
    "            \"\"\"here is just an example of what math things can be displayed using pylatex\"\"\"\n",
    "            a = np.array([[100, 10, 20]]).T\n",
    "            M = np.matrix([[2, 3, 4],\n",
    "                            [0, 0, 1],\n",
    "                            [0, 0, 2]])\n",
    "            doc.append(Math(data=['2*3', '=', 9]))\n",
    "            with doc.create(Tabular('rc|cl')) as table:\n",
    "                table.add_hline()\n",
    "                table.add_row((1, 2, 3, 4))\n",
    "                table.add_hline(1, 2)\n",
    "                table.add_empty_row()\n",
    "                table.add_row((4, 5, 6, 7))\n",
    "            doc.append(Math(data=[Matrix(M), Matrix(a), '=', Matrix(M * a)]))\n",
    "            with doc.create(Alignat(numbering=False, escape=False)) as agn:\n",
    "                agn.append(r'\\frac{a}{b} &= 0 \\\\')\n",
    "with doc.create(Section('Development Process')):\n",
    "    with doc.create(Subsection('Environment')):\n",
    "        with doc.create(Subsubsection('Framework')):\n",
    "            doc.append(NoEscape(r\"After early deliberation on the frameworks and games to work with, we decided upon Sonic the Hedgehog^\\(TM\\) as the game to train an agent on, and Stable-Retro, a ROM-loading and reinforcement learning toolkit. The game was chosen for its relative simplicity (gotta go fast!...to win), the fact that there preexisting benchmarks for games of its kind, and the fact that it is a continuous state game. Stable-Retro allows us to load and interact with the Sonic the Sonic the Hedgehog^\\(TM\\) ROM, as it provides a variety of preexisting tools for reading information from the game that is necessary for developing a RL training environment as well as providing reward to the agent. This approach allows us to focus on the setup for training and the training itself, removing the need for us to painstakingly target specific RAM values from the game process. Such vital information used to design the reward function and training restrictions include the current level, Sonic's position, and the number of lives.\"))\n",
    "        with doc.create(Subsubsection('Wrappers')):\n",
    "            doc.append(NoEscape(r'To simplify the training environment, Darroll used wrappers to preprocess a variety of features about the game state before it is fed into the algorithm. These wrappers are used to reduce the computational cost of training and to provide the agent with a more informative view of the environment. The following wrappers were implemented:'))\n",
    "            with doc.create(Paragraph('ObservationWrapper')):\n",
    "                doc.append(NoEscape(r\"\"))\n",
    "            with doc.create(Paragraph('FrameStack')):\n",
    "                doc.append(NoEscape(r\"This wrapper stacks the last four frames together to give the agent a sense of motion. This is important because it allows the agent to understand how the environment is changing over time. This was crucial to the agent's ability to learn how to navigate the level.\"))\n",
    "            with doc.create(Paragraph('Wrappr2')):\n",
    "                doc.append(NoEscape(r''))\n",
    "    with doc.create(Subsection(('Reward Function'))):\n",
    "        doc.append(NoEscape(r\"Using OpenAI's Stable-Baselines3 PPO implementation, Darroll abstracted the algorithm to focus on the training environment and the reward function. For context, Stable-Baselines3 is a library that provides a variety of reinforcement learning algorithms, including PPO. I should also mention that the metric used to generally determine the success of the agent are the win/loss ratio and the average level completion time. Furthermore, any mention of the quality of the agent's problem-solving & exploration abilities were essentially measured by observing two things from trained models: 1) if agents learned to jump over obstacles, which essentially measures problem solving ability, and 2) if agents learned to complete a circular loop in the level, which measures exploration ability. The latter was an interesting problem to solve, which will be discussed later but varied drastically from model-to-model depending on hyperparameter values and the reward function.\"))\n",
    "        with doc.create(Paragraph('Velocity')):\n",
    "            doc.append(NoEscape(r\"Initial tests with velocity-based rewards were unsuccessful. Calculated as delta x, velocity-based rewards led to reward farming as well as policies that preferred to move around in ways that acquired bursts of speed. Even after tuning the reward to account only for rightward velocity, the agent still failed to consistently beat the level. This may have been due to the fact that this reward had no relation to completing the level, which would would explain the agent's inability to learn how to do so.\"))\n",
    "        with doc.create(Paragraph('Progress')):\n",
    "            doc.append(NoEscape(r\"Based on the failures of the previous reward function as well as on preexisting research, I moved on to a progress-based reward function. This function rewarded the agent for moving rightward, which was a necessary condition for completing the level. This reward function was successful in training the agent to complete the level, but it was not able to train the agent to complete the loop.\"))\n",
    "            # We used the OpenAI Gym toolkit to create the training environment for our agent. The toolkit provides a variety of environments for training agents, including Sonic the Hedgehog^\\(TM\\). We used the Sonic the Hedgehog environment to train our agent to play the game.\n",
    "    with doc.create(Subsection('Pit of Despair')):\n",
    "        doc.append(NoEscape(r\"\"))\n",
    "    with doc.create(Subsection('Completing the Loop')):\n",
    "        doc.append(NoEscape(r\"\"))\n",
    "with doc.create(Section('GitHub')):\n",
    "    doc.append(r\"A repository containing the code used in this project can be found at:\")\n",
    "    doc.append(NewLine())\n",
    "    doc.append(NewLine())\n",
    "    doc.append(r\"https://github.com/Iemontine/ProximalPolicyOptimization.\")\n",
    "    # TODO: move this\n",
    "    with doc.create(Subsection('Cool graph')):\n",
    "        with doc.create(Figure(position='h!')) as img:\n",
    "            image_filename = \"Pass1-3_500k_-0.01punishment_2.jpg\"\n",
    "            img.add_image(image_filename, width='450px')\n",
    "            img.add_caption('Look at this photograph')\n",
    "doc.append(NewPage())  # Insert a page break\n",
    "with doc.create(Section('Contributions')):\n",
    "    with doc.create(Subsection('Darroll Saddi')):\n",
    "        doc.append(r'Set up training environment, including integrating libraries & frameworks. Performed inference- and intuition-based hyperparameter tuning to produce training results. Designed reward function & investigated multiple implementations including the multi-pass implementation. Implemented metric-tracking during training and wrote plotting code. Contributed to formal and GitHub documentation. Contributed research efforts and project direction.')\n",
    "    with doc.create(Subsection('Andrew Yeow')):\n",
    "        doc.append(r'Implemented proof of concept with DQL.')\n",
    "    with doc.create(Subsection('Steven Yi')):\n",
    "        doc.append(r\"Refactored OpenAI's proximal policy optimization implementation to suit our purposes.\")\n",
    "    with doc.create(Subsection('Christine Morayata')):\n",
    "        doc.append(r'Contributed to documentation.')\n",
    "    with doc.create(Subsection('Julia Heiler')):\n",
    "        doc.append(r'Contributed to documentation.')\n",
    "    with doc.create(Subsection('Ryan Li')):\n",
    "        doc.append(r'Contributed to documentation.')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        doc.generate_pdf('output')\n",
    "        print(\"Completed!\")\n",
    "        break\n",
    "    except:\n",
    "        clear_output()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
