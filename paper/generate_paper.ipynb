{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "from ast import Sub\n",
    "from math import e\n",
    "from pylatex.utils import italic, bold, NoEscape\n",
    "from pylatex import Document, Section, Subsection, Subsubsection, Description, Itemize, Command, Tabular, Math, TikZ, Axis, Plot, Figure, Matrix, Alignat, NewPage, NewLine, Enumerate\n",
    "from pylatex.section import Paragraph, Subparagraph, Chapter\n",
    "from pylatex.utils import italic\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "# =================================[PAGE SETUP]=================================\n",
    "geometry_options = { \"margin\": \"0.8in\", \"includeheadfoot\": False }\n",
    "doc = Document(geometry_options=geometry_options, lmodern = True)\n",
    "# ====================================[TITLE]===================================\n",
    "doc.preamble.append(Command('title', NoEscape(r'Gameplaying AI: Implementing Proximal Policy Optimization\\\\ \\large Introduction to Artificial Intelligence Project\\\\ \\large ECS 170 Fall 2024')))\n",
    "doc.preamble.append(Command('author', NoEscape(r'Darroll Saddi^\\(1\\), Andrew Yeow^\\(1\\), Christine Morayata^\\(?\\), Julia Heiler^\\(?\\), Ryan Li^\\(1\\), Steven Yi^\\(?\\)\\\\ \\small^\\(1\\)University of California, Davis - Computer Science\\\\\\small^\\(2\\)University of California, Davis - Cognitive Science')))\n",
    "doc.preamble.append(Command('date', NoEscape(r'\\today')))\n",
    "doc.append(NoEscape(r'\\maketitle'))\n",
    "# ==================================[ABSTRACT]==================================\n",
    "doc.append(NoEscape(r'\\begin{abstract}This report documents a retrospective reimplementation of proximal policy optimization (PPO), for study purposes. Our goal was to not only use the algorithm to succesfully train a model to play Sonic the Hedgehog^\\(TM\\), but to additionally discover how to use, articulate, and implement reinforcement learning algorithms while being able to communicate PPO and reinforcement learning through practical experience. We also present this report as an insightful and educational resource for those interested in reinforcement learning and/or recreating our training environment.\\end{abstract}'))\n",
    "# ================================[INTRODUCTION]================================\n",
    "with doc.create(Section('Introduction')):\n",
    "    doc.append(NoEscape(r'Reinforcement learning (RL) has emerged as a powerful paradigm for training agents to make sequential decisions by interacting with their environment. One of the most notable advancements in this field is Proximal Policy Optimization (PPO), a versatile algorithm that has shown significant promise across various domains. This report documents the extensive and arduous process of utilizing OpenAI’s implementation of PPO to train an agent to play the classic video game, Sonic the Hedgehog^\\(TM\\), with a particular focus on developing an effective reward function. The dual objectives of this project were to successfully leverage PPO for training the game-playing model and to deepen our understanding of RL algorithms through practical implementation and analysis. The motivation for choosing Sonic the HedgehogTM as the training environment stems from the game’s continuous state space and the existence of pre-established benchmarks, which facilitate comparative analysis. Additionally, the relatively simple mechanics of the game make it an ideal candidate for exploring and demonstrating the efficacy of PPO in a controlled setting. Through this project, we aim to provide an insightful and educational resource for the practical challenges associated with implementing PPO. By documenting our development process, challenges, and solutions, we hope to contribute to the broader understanding of reinforcement learning.'))\n",
    "    # doc.append(NoEscape(r'Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that has been used to train agents in continuous action space environments, including video games. It is a policy gradient method that is designed to be simple to implement and computationally efficient. Being a on-policy method, the algorithm learns a policy to make decisions in the environment. This is different from an off-policy method, which learns the value of the optimal policy independently of the agent\\'s action. In this project, we reimplemented PPO and used it to train an agent to play and complete levels in Sonic the Hedgehog^\\(TM\\). On a high level, PPO works by choosing an action for the agent to take, then observing the resultant state and reward. Then, an estimate for the advantage gain is computed (see GAE), which measures how much better the action was compared to the average action at that state. The policy is then updated using a special objective function to prevent the policy from updating too mcuh on a single episode by penalizing if the new policy deviates too much from the original policy, ensuring stability. These steps repeat until training is complete. Here is an example of a citation\\footnote{This is a reference}. Here is another reference\\footnote{This is another reference}. This is how I would cite the first reference again^\\(1\\).'))\n",
    "    # More context on PPO needed above\n",
    "    # TODO: Be sure to draw on multiple sources for this introduction. Explain motivation for project.\n",
    "# ================================[BACKGROUND]==================================\n",
    "with doc.create(Section('Background')):\n",
    "    with doc.create(Subsection('Use-Cases')):\n",
    "        doc.append(NoEscape(r'Proximal policy optimization (PPO) is a versatile reinforcement learning algorithm that can be used in a variety of environments. It is particularly well-suited for continuous state spaces, which makes it a good candidate for training agents in games like Sonic the Hedgehog^\\(TM\\).'))\n",
    "        # TODO: Talk about the build-up to PPO, and how it excels over its predecessors.\n",
    "    with doc.create(Subsection('PPO Implementation')):\n",
    "        # TODO: PLS REWRITE + ADD REFERENCES\n",
    "        with doc.create(Paragraph('Actor-Critic')):\n",
    "            doc.append(NoEscape(r'PPO is an actor-critic algorithm, which means that it uses two neural networks to approximate the policy and value functions. The actor network takes the current state as input and outputs a probability distribution over possible actions. The critic network takes the current state as input and outputs an estimate of the value of the state. Both use the metric of \\(advantage\\) to update weights, which measures how much better taking a particular action is compared to the average action.'))\n",
    "        with doc.create(Paragraph('Generalized Advantage Estimation (GAE)')):\n",
    "            doc.append(NoEscape(r\"This function meaasures how good the current state is. It is used to calculate the advantage of taking an action in a given state, which is then used to update the policy. The advantage is a measure of how much better an action is than the average action, and it is used to determine how to update the policy to improve performance.\"))\n",
    "        with doc.create(Paragraph('Surrogate Objective Function')):\n",
    "            doc.append(NoEscape(r'The key to PPO is the surrogate objective function, which helps maximize the probability of taking an action that may eventually lead to high reward. Its main purpose is to keep updates within a trust region, using clipping.'))\n",
    "        with doc.create(Paragraph('Clipping')):\n",
    "            doc.append(NoEscape(r'This is a technique used to prevent the policy from changing too much between updates. It is used to ensure that the policy does not change too much between updates, which would normally lead to training instability. It does this by clipping the probability ratio between the new and old policies.'))\n",
    "            # TODO: invest in some visualization\n",
    "            # TODO: include an image that connects every part of the algorithm\n",
    "            \"\"\"here is just an example of what math things can be displayed using pylatex\"\"\"\n",
    "            # a = np.array([[100, 10, 20]]).T\n",
    "            # M = np.matrix([[2, 3, 4],\n",
    "            #                 [0, 0, 1],\n",
    "            #                 [0, 0, 2]])\n",
    "            # doc.append(Math(data=['2*3', '=', 9]))\n",
    "            # with doc.create(Tabular('rc|cl')) as table:\n",
    "            #     table.add_hline()\n",
    "            #     table.add_row((1, 2, 3, 4))\n",
    "            #     table.add_hline(1, 2)\n",
    "            #     table.add_empty_row()\n",
    "            #     table.add_row((4, 5, 6, 7))\n",
    "            # doc.append(Math(data=[Matrix(M), Matrix(a), '=', Matrix(M * a)]))\n",
    "            # with doc.create(Alignat(numbering=False, escape=False)) as agn:\n",
    "            #     agn.append(r'\\frac{a}{b} &= 0 \\\\')\n",
    "with doc.create(Section('Development Process')):\n",
    "    with doc.create(Subsection('Environment')):\n",
    "        with doc.create(Subsubsection('Framework')):\n",
    "            doc.append(NoEscape(r\"After discovering Gym-Retro, which offers a variety of learning scenarios to choose from, we developed a proof-of-concept of training with a different algorithm (Deep Q-Learning). However, due to the outdatedness of this library and all its dependencies, we opted to search for a different approach that supported our desired versions of Python, Pytorch, and Gymnasium (RL environment loader). After deliberation on the frameworks and games to work with, we decided upon Sonic the Hedgehog^\\(TM\\) as the game to train an agent on, and Gymnasium x Stable-Retro, a ROM-loading and RL framework. This game was chosen for its relative simplicity (gotta go fast!..to win), its continuous action space environment, and the fact that there pre-existing benchmarks for games of its kind. Stable-Retro allows us to load and interact with video game ROMs as the training environment, and provides a variety of pre-existing tools for reading information from retro video games. These are necessary for developing an RL training environment as well as providing reward to the agent. Not only do these libraries have better dependency support, this approach also allowed us to focus on the setup for training and the training itself, removing the need for us to painstakingly target specific RAM values from the game process. Such vital information used to design the reward function and training restrictions include the current level, Sonic’s position, and the number of lives.\"))\n",
    "        with doc.create(Subsubsection('Wrappers')):\n",
    "            doc.append(NoEscape(r'To simplify the training environment, Darroll used wrappers to preprocess a variety of features about the game state before it is fed into the algorithm. These wrappers are used to reduce the computational cost of training and to provide the agent with a more informative view of the environment. The following wrappers were implemented:'))\n",
    "            with doc.create(Description()) as desc:\n",
    "                desc.add_item(\"Rewarder\", r\"Allows the specification of a custom reward function, in addition to timing resets (e.g. go back to start of level, reset all tracked variables, and administer negative reward when Sonic dies).\")\n",
    "                desc.add_item(\"Observer\", r\"Feeds what is on the screen into a convolutional neural network to easily map the game state to agent, as opposed to manually reading the game state from the ROM. This proved to be crucial for training, as it allowed for agents to learn an association between map features and an action to respond with.\")\n",
    "                desc.add_item(\"Frame Stacker\", r\"Stacks the last 8 frames together to teach a mapping between current and previous states (extremely crucial, allows for learning of jumping timing/early jumping over obstacles).\")\n",
    "                desc.add_item(\"Action Mapper\", r\"Discretizes the action space to a limited number of one-hot-encoded actions (e.g. move right, jump, move left), as opposed to completely random controller input combinations, thereby decreasing input complexity and allowing for faster learning.\")\n",
    "                desc.add_item(\"Frame Skipper\", r\"Periodically skips game frames to allow for faster training, further reducing complexity as less frames are processed overall.\")\n",
    "                desc.add_item(\"Multiprocess Vectorizer\", r\"Allows for training multiprocessing, which speeds up training by allowing for multiple agent instances to investigate solutions to obstacles concurrently.\")\n",
    "    with doc.create(Subsection(('Reward Function'))):\n",
    "        doc.append(NoEscape(r\"Using OpenAI’s Stable-Baselines3 version of PPO, I abstracted the algorithm to focus on the training environment and the reward function. For context, Stable-Baselines3 is a library that provides a variety of implementations of reinforcement learning algorithms, including PPO.\"))\n",
    "        with doc.create(Subsubsection('Note about Evaluation Metrics')):\n",
    "            doc.append(NoEscape(r\"I should also mention that the metric I used to estimate the success of the agent are the win/loss ratio and the average level completion time. Furthermore, any mention of the quality of the agent’s problem-solving \\& exploration abilities were essentially measured by observing two things from trained models:\"))\n",
    "            with doc.create(Enumerate()) as enum:\n",
    "                enum.add_item(r\"If the agent learned to jump over obstacles, which I used as a measure of the divide between problem solving vs solution optimization (exploration vs exploitation).\")\n",
    "                enum.add_item(r\"If the agent learned to complete a circular loop in the level, which I used to measure exploration ability.\")\n",
    "            doc.append(r\"The latter was an interesting problem to solve, which will be discussed later but varied drastically from model-to-model depending on hyperparameter values and the reward function.\")\n",
    "        with doc.create(Paragraph('Velocity')):\n",
    "            doc.append(NoEscape(r\"Initial tests with velocity-based rewards were unsuccessful. Calculated as $\\Delta x$, velocity-based rewards led to reward farming as well as policies that preferred to move around in ways that acquired bursts of speed. Even after tuning the reward to account only for rightward velocity, the agent still failed to consistently beat the level. This may have been due to the fact that this reward had a small relation to completing the level, which would would explain the agent's inability to learn how to do so. In other words, without a proper problem statement, the problem could be solved in unexpected ways.\"))\n",
    "        with doc.create(Paragraph('Progress')):\n",
    "            doc.append(NoEscape(r\"Based on the failures of the previous reward function as well as on preexisting research, I moved on to a progress-based reward function. This function rewards the agent for making rightward progress through the level, and is a better heuristic as it actually relates to the task of completing the level. This reward function was successful in training the agent to complete the level, but it was not able to train the agent to complete the loop. It was at this point where I additionally implemented punishments (negative rewards) for the agent losing a life, as well as massive static rewards for completing the level. This reward function yielded good results, and the agent was able to at least make progress up to the loop. However, I observed the agents getting stuck in a local minimum by repeatedly jumping rightwards into the loop, and would never learn how to complete it. I believe this was due to agents entering an 'exploitation' phase of training too early, and would continue to repeat the behavior of 'jumping to the right' that was successfully solving most obstacles before the loop. It was clear I needed to tune the hyperparameters to increase the agent's exploration ability.\"))\n",
    "    with doc.create(Subsection('Hyperparameter Tuning')):\n",
    "        doc.append(NoEscape(r\"A roadblock in development was figuring out good hyperparameters and reward magnitudes to train the model with. Depending on the values chosen, the outcomes varied greatly. At some point, I also spent time investigating using the Optuna library to perform automated hyperparameter tuning, but due to the sheer complexity and required length of training (several hours), this approach was not feasible under time constraints as it would require multiple full training cycles to determine the optimized values. What follows is a brief description of what values I converged upon, and my intuition for why they worked.\"))\n",
    "        with doc.create(Description()) as desc:\n",
    "            desc.add_item(\"Discount factor\", r\"0.95, arbitrarily chosen from training standards\")\n",
    "            desc.add_item(\"Bias trade-off vs variance factor\", r\"0.985, arbitrarily chosen from training standards\")\n",
    "            desc.add_item(\"Clipped surrogate objective\", r\"0.75, arbitrarily chosen from training standards\")\n",
    "            desc.add_item(\"Entropy\", r\"0.30, mild magnitude to encourage random actions to explore solutions to completing the loop, without being excessively high which prevented the agent from learning from successes.\")\n",
    "            desc.add_item(\"Learning Rate\", r\"0.00075, values that were too high (0.1, 0.001, 0.0001) caused the agent to enter the exploitation phase of training too early, evident by the agent performing at a local maximum (preferring to get stuck at the loop to farm progress rewards during the reset from taking too long). Values that were too low (0.00001) decreased learning speed without significant benefit, so a slightly higher value was chosen to increase speed while maintaining exploration.\")\n",
    "            desc.add_item(\"Timesteps before updating policy\", r\"512, this value measures the number of timesteps per agent before the policy is updated. 4096 was originally used, as I believed this would allow the agent longer stretches of forward progress with large amounts of positive reward. Although agents learned to beat early areas of the map consistently after around 500,000 timesteps, I discovered that lowering the value to 512 decreased this to a mere 50,000-100,000 timesteps. I intepreted this as being due to each section of the map essentially now being solved in smaller pieces with 512, whereas with 4096, much larger portions of the level are presented to the neural network as larger problems.\")\n",
    "    with doc.create(Subsection('Multi-Pass Training')):\n",
    "        with doc.create(Subsubsection('Adjusting Reward/Punishment Weights')):\n",
    "            doc.append(NoEscape(r\"At this point in development, I arbitrarily chose weights for agent progress rewards, death punishments, and level complete rewards. As it turns out however, these each had massive impacts on the quality of training and produced widely varying models. Depending on these magnitudes, policies could develop that never learned to complete the loop, opting again to farm the initial progress rewards from starting the level, or even in the worst-case of my observation, be completely unable to learn to jump over obstacles. Based on this, I lowered the progress rewards to be calculated as $\\Delta x / 100$. This produces rewards as decimals in the range of 0.0 and 0.5. The resultant training was faring far better, and it seemed that keeping rewards low, but not insignificant, was important for agents to develop a mapping between making forward progress and getting closer to the true goal which is the end of the level. Level completion reward and death punishment magnitudes converged on being set to 50 and -1 respectively, both being subject to change as I continued my optimization efforts.\"))\n",
    "        with doc.create(Subsubsection('Multi-Pass')):\n",
    "            doc.append(r\"At this point, I was getting results, but they weren't satisfactory.\")\n",
    "            with doc.create(Figure(position='h!')) as fig:\n",
    "                image_filename = \"Pass1_15M_1.png\"\n",
    "                fig.add_image(image_filename, width='500px')\n",
    "                fig.add_caption('Winrate, completion time progression after 1.5 million timesteps, one-shot training')\n",
    "            doc.append(r\"Figure 1 shows that as training progressed, the winrate, even after a period of volatile failure that occurs from early training in which the likelihood of losing a life is very high, ranges between 43.94% and 49.33%. In other words, by the end of training the model with the above reward/punishment magnitudes, the resultant policy is successful less than roughly 50% of the time. Furthermore when it is able to complete the level, the 5 lowest completion times are relatively low. However, the average time is very poor, at around 1 minute and 50 seconds, which means completion times are inconsistent given the lower and middle bounds. Keep in mind the world record completion of this level by a human is around 25 seconds. This led me to develop a new reward function, working as follows:\")\n",
    "            with doc.create(Paragraph('Pass 1 (500k timesteps), original setup')):\n",
    "                with doc.create(Itemize()) as itemize:\n",
    "                    itemize.add_item(NoEscape(r'Reward for progress = $\\Delta x / 100$, where x is the x-position in map'))\n",
    "                    itemize.add_item('Reward for winning = 50')\n",
    "                    itemize.add_item('Punishment for death = -1')\n",
    "                    itemize.add_item(NoEscape(r'\\textbf{NEW}: Punishment for not making progress = 0.00'))\n",
    "            with doc.create(Paragraph('Pass 2 (500k timesteps)')):\n",
    "                with doc.create(Itemize()) as itemize:\n",
    "                    itemize.add_item(NoEscape(r'Reward for progress = $\\Delta x / 100$'))\n",
    "                    itemize.add_item('Reward for winning = 50')\n",
    "                    itemize.add_item('Punishment for death = -1')\n",
    "                    itemize.add_item(NoEscape(r'Punishment for not making progress = \\textbf{-0.01}'))\n",
    "            with doc.create(Paragraph('Pass 3 (500k timesteps)')):\n",
    "                with doc.create(Itemize()) as itemize:\n",
    "                    itemize.add_item('Same as Pass 2, just ran again to match 1.5M timestep arbitrary limit')\n",
    "            doc.append(NoEscape(r\"\"\"At this point, I was skeptical of an implementation that 'hard-coded' a learning progression, as opposed training with an ideal reward function and ideal hyperparameters. I only took this approach because I thought at the time that punishing on progress was not possible due to initial experimental failures that turned out to be due to poor reward magnitudes and hyperparemter choices. My intuition for what was wrong, was that the setup lacked a heuristic for the learning curve a human might traverse when learning to beat a video game level quickly; the agents were able to learn to beat the level, but would rarely optimize for time, which I believed could be achieved by punishing on a lack of progress once the agent knew how to beat the level. While this proved to be wrong, here were the results:\"\"\"))\n",
    "            # TODO: re-render these plots with updated titles n such\n",
    "            with doc.create(Figure(position='h!')) as fig:\n",
    "                image_filename = \"multipass_500k_1.png\"\n",
    "                fig.add_image(image_filename, width='500px')\n",
    "            with doc.create(Figure(position='h!')) as fig:\n",
    "                image_filename = \"multipass_500k_2.png\"\n",
    "                fig.add_image(image_filename, width='450px')\n",
    "                fig.add_caption('multipass implementation; winrate progression, winrate frequency, overall winrate')\n",
    "            doc.append(r\"Figure 2 provides context on how each of the three pass's winrates and win frequencies compare next to each other, as well as how many wins/losses each pass was able to obtain. Additionally, by comparing Figure 1 and Figure 3, it is clear that the performance of this 3-Pass implementation is a significant improvement. The winrate, stabilized after ignoring the volatile exploration phase (estimated to be within the first 600 wins/losses) is at least 82.94%. Compared to the 49.33% from the preceding setup, this is a 68.13% improvement in winrate, which is quite significant. To prove this methodology is in any way ideal, I decided to attempt training 1.5M timesteps for each component reward function separately, in order to examine their performance and draw comparisons, here are the results:\")\n",
    "            #  This is where I realized that the underlying problem was that reward function used in Pass 1 was simply poorly designed, all on the basis of the whether a lack of progress was punished.\n",
    "            with doc.create(Figure(position='h!')) as fig:\n",
    "                image_filename = \"punish_v_nopunish_levelcompletion.png\"\n",
    "                fig.add_image(image_filename, width='330px')\n",
    "            with doc.create(Figure(position='h!')) as fig:\n",
    "                image_filename = \"punish_v_nopunish_winrates.png\"\n",
    "                fig.add_image(image_filename, width='330px')\n",
    "            with doc.create(Figure(position='h!')) as fig:\n",
    "                image_filename = \"punish_v_nopunish_wintimes.png\"\n",
    "                fig.add_image(image_filename, width='340px')\n",
    "                fig.add_caption('Level completion time and frequency with and without punishing on lack of progress')\n",
    "            doc.append(NewPage())\n",
    "            doc.append(r\"Resultant winrate from training while punishing lack of progress the entire time, compared to training in multiple passes with varying reward functions, is equal to if not slightly better than the alternative winrate. Similarly, the number of wins that occur is also slightly better, at around an icnrease of ~300 wins over the same duration of timesteps. And finally, the resultant wintimes at the end of training are at fastest 31.54, which is approximately the same as it was with multi-pass, and the average completion time saw an improvement of about 4 seconds. This is significant, as it means that me hard-coding a learning progression may have been unnecessary.\")\n",
    "with doc.create(Section('Results/Reflections')):\n",
    "    with doc.create(Subsection('Reinforcement Learning as True Learning')):\n",
    "        doc.append(NoEscape(r\"Throughout the development of this project, I often intuited how to produce more efficient training by referencing how the agent was performing in the environment, in other words observing the agent’s behavior and adjusting hyperparameters accordingly. For example, I noticed that the agent was getting stuck at the loop, so I increased the entropy to encourage more exploration of the problem and decreased the number of timesteps before updating the policy to allow the agent to learn to beat the level in smaller pieces. Another example was how I designed the reward function. I often thought of how a human might learn to beat a level of a video game quickly. I imagined they would traverse a learning curve, first needing to learn to beat the level, then optimizing after that and self-punishing when dying or not moving faster. This led me to implement the multi-pass implementation that produced positive results. Although the multi-pass method proved to be worse than one-shot generally, it was what led me back to one-shot training in the first place. This goes to show the power of reinforcement learning algorithms like PPO to mimic true learning, likely going through the same learning curve that I attempted to hard code, in its own way. These high-level, ’black-box’ observations were crucial to the success of the project, as they allowed me to find intuitive learning heuristics, leading to the 90\\% accuracy of the model. It is also clear that the focus should undoubtedly be on training and learning, rather than attempting to hard-code human-centric implementation. I believe that similar simplifications and abstractions of the inner workings of PPO and similar algorithms must be taken in other reinforcement learning projects. I further believe that this is a testament to the power of reinforcement learning as an algorithm reflecting true learning.\"))\n",
    "        # TODO: test training with pass 1 only for 1,500,000 timesteps and pass 3 for 1,500,000 timesteps\n",
    "with doc.create(Section('GitHub')):\n",
    "    doc.append(r\"A repository containing the code used in this project can be found at:\")\n",
    "    doc.append(NewLine())\n",
    "    doc.append(NewLine())\n",
    "    doc.append(r\"https://github.com/Iemontine/ProximalPolicyOptimization.\")\n",
    "doc.append(NewPage())  # Insert a page break\n",
    "with doc.create(Section('Contributions')):\n",
    "    with doc.create(Subsection('Darroll Saddi')):\n",
    "        doc.append(r'Set up training environment, including integrating libraries & frameworks. Performed inference- and intuition-based hyperparameter tuning to produce training results. Designed reward function, investigating and testing multiple implementation styles. Implemented metric-tracking during training and wrote plotting code. Contributed to LaTeX write-up and GitHub documentation. Contributed research efforts and project direction.')\n",
    "    with doc.create(Subsection('Julia Heiler')):\n",
    "        doc.append(r'Contributed to documentation; presentation, write-ups, slide deck, ensuring coherence and cohesion throughout. Contributed research efforts and project direction.')\n",
    "    with doc.create(Subsection('Andrew Yeow')):\n",
    "        doc.append(r'Implemented proof of concept with DQL.')\n",
    "    with doc.create(Subsection('Steven Yi')):\n",
    "        doc.append(r\"Refactored OpenAI's proximal policy optimization implementation to suit our purposes.\")\n",
    "    with doc.create(Subsection('Christine Morayata')):\n",
    "        doc.append(r'Contributed to documentation.')\n",
    "    with doc.create(Subsection('Ryan Li')):\n",
    "        doc.append(r'Contributed to documentation.')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        doc.generate_pdf('output')\n",
    "        print(\"Completed!\")\n",
    "        break\n",
    "    except:\n",
    "        clear_output()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
